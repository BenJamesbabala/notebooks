{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Logistic Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting /tmp/data/train-images-idx3-ubyte.gz\n",
      "Extracting /tmp/data/train-labels-idx1-ubyte.gz\n",
      "Extracting /tmp/data/t10k-images-idx3-ubyte.gz\n",
      "Extracting /tmp/data/t10k-labels-idx1-ubyte.gz\n"
     ]
    }
   ],
   "source": [
    "%reset -f\n",
    "import tensorflow as tf\n",
    "tf.reset_default_graph()\n",
    "from tensorflow.examples.tutorials.mnist import input_data\n",
    "mnist = input_data.read_data_sets(\"/tmp/data/\", one_hot=True)\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First of all we need to create `placeholder` objects for our data. These are objects that we put inside the default Tensorflow Graph. The first point in our execution sequence."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "X = tf.placeholder(tf.float32, shape = [None, 784])\n",
    "Y = tf.placeholder(tf.float32, shape = [None, 10])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "These are empty matrices which can be populated with data later on. `X` will contain our vectorised images, and `Y` the one hot class labels."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we can create similar placeholders for the model weights, termed `Variable` objects in Tensorflow. These will be initialized to random normal values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "W = tf.Variable(tf.random_normal([784, 10]))\n",
    "b = tf.Variable(tf.random_normal([1,10]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we form our predictions of what y is based on our weights W and the bias term b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "y = tf.matmul(X,W) + b"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "How close are we to the predictions? We need a definition of logistic error. Since each image only has one class, the total probability over all the class is 1. Therefore we can use the `softmax` function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "cost = tf.reduce_sum(tf.nn.softmax_cross_entropy_with_logits(y,Y))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we need to define the optimiser. Tensorflow handles much of the work here, we need only to specify the type of backpropagation we require. For example we could choose the the `GradientDescentOptimizer` or the `AdamOptimizer`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "optimiser = tf.train.GradientDescentOptimizer(0.01).minimize(cost)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It also would be quite useful if we were also able to test the accuracy of our model on unseen data, for this we need a measure of accuracy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "accuracy = tf.reduce_mean(tf.cast(tf.equal(tf.arg_max(y,1), tf.arg_max(Y,1)), tf.float32))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Before we train, we need to initialize all our variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "init = tf.initialize_all_variables()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we can train our network"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We are using <b>Stochastic Gradient Descent</b>, this means that batches of input are fed into the network, and the network weights update accordingly. A single run through all the training instances is known as a full <b>epoch</b>. We will be using batches of size 100, and so the number of batches per epoch is the total number of training examples (55000), divided by the batch size (100), which equals 550."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "550\n"
     ]
    }
   ],
   "source": [
    "batch_size = 100\n",
    "n_training = len(mnist.train.labels)\n",
    "n_batches = n_training/batch_size\n",
    "print n_batches"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "55000"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(mnist.train.labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cost: 44.4421463013, Accuracy: 0.864500105381\n",
      "Cost: 59.498714447, Accuracy: 0.879700124264\n",
      "Cost: 33.9364624023, Accuracy: 0.898000121117\n",
      "Cost: 39.7270812988, Accuracy: 0.898500084877\n",
      "Cost: 50.8886604309, Accuracy: 0.902200102806\n",
      "Cost: 39.3054199219, Accuracy: 0.904600024223\n",
      "Cost: 63.7918243408, Accuracy: 0.907300114632\n",
      "Cost: 24.2948684692, Accuracy: 0.905900120735\n",
      "Cost: 60.6184577942, Accuracy: 0.909900128841\n",
      "Cost: 45.2948265076, Accuracy: 0.914600133896\n",
      "Cost: 30.7770385742, Accuracy: 0.914900124073\n",
      "Cost: 25.4723052979, Accuracy: 0.916400134563\n",
      "Cost: 22.3743152618, Accuracy: 0.909700155258\n",
      "Cost: 42.5977516174, Accuracy: 0.911200106144\n",
      "Cost: 35.0974197388, Accuracy: 0.90180015564\n",
      "Cost: 33.2313575745, Accuracy: 0.904500126839\n",
      "Cost: 12.0502147675, Accuracy: 0.917600095272\n",
      "Cost: 40.4304847717, Accuracy: 0.916300058365\n",
      "Cost: 50.6873168945, Accuracy: 0.908900022507\n",
      "Cost: 40.7444076538, Accuracy: 0.917300105095\n"
     ]
    }
   ],
   "source": [
    "with tf.Session() as sess:\n",
    "    sess.run(init)\n",
    "    for ep in range(20):\n",
    "        for b in range(n_batches):\n",
    "            b_images, b_labels = mnist.train.next_batch(batch_size)\n",
    "            _, c = sess.run([optimiser, cost], feed_dict={X:b_images, Y:b_labels})\n",
    "        acc = sess.run(accuracy, feed_dict={X: mnist.test.images, Y: mnist.test.labels})\n",
    "        print \"Cost: {}, Accuracy: {}\".format(c, acc)\n",
    "        \n",
    "        \n",
    "        \n",
    "        \n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we can see, we don't do too badly. Our classifier is 91.8% accurate, however we can do much better. This is equivalent to doing logistic regression, which is still a linear model. We will be able to build non-linearity by combining multiple layers and non-linear activation functions."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [Root]",
   "language": "python",
   "name": "Python [Root]"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
