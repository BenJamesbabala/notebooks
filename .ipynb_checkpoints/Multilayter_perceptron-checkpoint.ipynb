{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting /tmp/data/train-images-idx3-ubyte.gz\n",
      "Extracting /tmp/data/train-labels-idx1-ubyte.gz\n",
      "Extracting /tmp/data/t10k-images-idx3-ubyte.gz\n",
      "Extracting /tmp/data/t10k-labels-idx1-ubyte.gz\n"
     ]
    }
   ],
   "source": [
    "%reset -f\n",
    "import tensorflow as tf\n",
    "from tensorflow.examples.tutorials.mnist import input_data\n",
    "mnist = input_data.read_data_sets(\"/tmp/data/\", one_hot=True)\n",
    "%matplotlib inline\n",
    "\n",
    "X = tf.placeholder(tf.float32, shape = [None, 784])\n",
    "Y = tf.placeholder(tf.float32, shape = [None, 10])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To implement a multilayer perception, we need much the same code as we had before. The data, cost functions and training all identical, however we modify the model. Before we had our predictions as a linear combination of our input matrix $X$, with a matrix of weights $W$ plus a bias term $b$.\n",
    "\n",
    "`y = tf.matmul(X,W) + b`\n",
    "\n",
    "Now we will add a hidden layer to our network and a non-linear activation function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "n_hidden_units = 200\n",
    "W1 = tf.Variable(tf.random_normal([784, n_hidden_units]))\n",
    "b1 = tf.Variable(tf.random_normal([1,n_hidden_units]))\n",
    "hidden_layer = tf.nn.relu(tf.matmul(X,W1) + b1)\n",
    "\n",
    "W2 = tf.Variable(tf.random_normal([n_hidden_units, 10]))\n",
    "b2 = tf.Variable(tf.random_normal([1,10]))\n",
    "\n",
    "y = tf.matmul(hidden_layer, W2) + b2\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The rest of the code is exactly the same as before."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "cost = tf.reduce_sum(tf.nn.softmax_cross_entropy_with_logits(y,Y))\n",
    "\n",
    "optimiser = tf.train.GradientDescentOptimizer(0.01).minimize(cost)\n",
    "\n",
    "accuracy = tf.reduce_mean(tf.cast(tf.equal(tf.arg_max(y,1), tf.arg_max(Y,1)), tf.float32))\n",
    "\n",
    "init = tf.initialize_all_variables()\n",
    "\n",
    "batch_size = 100\n",
    "n_training = len(mnist.train.labels)\n",
    "n_batches = n_training/batch_size\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cost: 69.1936950684, Accuracy: 0.827600002289\n",
      "Cost: 31.7392158508, Accuracy: 0.887900114059\n",
      "Cost: 41.723815918, Accuracy: 0.901500046253\n",
      "Cost: 40.95262146, Accuracy: 0.857900083065\n",
      "Cost: 37.554977417, Accuracy: 0.916000187397\n",
      "Cost: 32.255355835, Accuracy: 0.92030018568\n",
      "Cost: 28.7401027679, Accuracy: 0.904700160027\n",
      "Cost: 22.2487411499, Accuracy: 0.919200062752\n",
      "Cost: 13.6244020462, Accuracy: 0.927500069141\n",
      "Cost: 11.9366807938, Accuracy: 0.928900122643\n",
      "Cost: 8.46895694733, Accuracy: 0.93330013752\n",
      "Cost: 31.6141433716, Accuracy: 0.931500077248\n",
      "Cost: 12.6586475372, Accuracy: 0.93590015173\n",
      "Cost: 13.5284967422, Accuracy: 0.937900066376\n",
      "Cost: 33.2912635803, Accuracy: 0.932800114155\n",
      "Cost: 12.1083459854, Accuracy: 0.936200082302\n",
      "Cost: 15.8711204529, Accuracy: 0.939400017262\n",
      "Cost: 11.9828901291, Accuracy: 0.939500212669\n",
      "Cost: 12.5972557068, Accuracy: 0.932800114155\n",
      "Cost: 11.4336891174, Accuracy: 0.939900100231\n"
     ]
    }
   ],
   "source": [
    "with tf.Session() as sess:\n",
    "    sess.run(init)\n",
    "    for ep in range(20):\n",
    "        for b in range(n_batches):\n",
    "            b_images, b_labels = mnist.train.next_batch(batch_size)\n",
    "            _, c = sess.run([optimiser, cost], feed_dict={X:b_images, Y:b_labels})\n",
    "        acc = sess.run(accuracy, feed_dict={X: mnist.test.images, Y: mnist.test.labels})\n",
    "        print \"Cost: {}, Accuracy: {}\".format(c, acc)\n",
    "        \n",
    "        \n",
    "        \n",
    "        \n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we can see, we don't do too badly. Our classifier is 91.8% accurate, however we can do much better. This is equivalient to doing logistic regression, which is still a linear model. We will be able to build non-linearity by combining multiple models and non-linear activation functions."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "We can see that this code does better than before, achieving almost 94%"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [Root]",
   "language": "python",
   "name": "Python [Root]"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
